{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Train-data-load\" data-toc-modified-id=\"Train-data-load-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Train data load</a></div><div class=\"lev2 toc-item\"><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data preparation</a></div><div class=\"lev3 toc-item\"><a href=\"#Imputting-missing-values\" data-toc-modified-id=\"Imputting-missing-values-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Imputting missing values</a></div><div class=\"lev1 toc-item\"><a href=\"#Model-selection\" data-toc-modified-id=\"Model-selection-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model selection</a></div><div class=\"lev2 toc-item\"><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Feature selection</a></div><div class=\"lev2 toc-item\"><a href=\"#Evaluating--single-classifiers\" data-toc-modified-id=\"Evaluating--single-classifiers-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Evaluating  single classifiers</a></div><div class=\"lev3 toc-item\"><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-221\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Random Forest</a></div><div class=\"lev3 toc-item\"><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-222\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Logistic Regression</a></div><div class=\"lev3 toc-item\"><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-223\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>XGBoost</a></div><div class=\"lev1 toc-item\"><a href=\"#Predictions-on-test-set\" data-toc-modified-id=\"Predictions-on-test-set-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predictions on test set</a></div><div class=\"lev2 toc-item\"><a href=\"#Missing-values-imputation-and-feature-engineering\" data-toc-modified-id=\"Missing-values-imputation-and-feature-engineering-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Missing values imputation and feature engineering</a></div><div class=\"lev2 toc-item\"><a href=\"#Predict-on-test-and-output-submission\" data-toc-modified-id=\"Predict-on-test-and-output-submission-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Predict on test and output submission</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sg0892454\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import ttest_ind \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, make_scorer, roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('datasets' + os.sep + 'train.csv', na_values=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595212, 59)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_rows = d.shape[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin  ps_ind_14  ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  ps_reg_02  ps_reg_03  ps_car_01_cat  ps_car_02_cat  ps_car_03_cat  ps_car_04_cat  ps_car_05_cat  ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  ps_car_09_cat  ps_car_10_cat  ps_car_11_cat  ps_car_11  ps_car_12  ps_car_13  ps_car_14  ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  ps_calc_04  ps_calc_05  ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  ps_calc_10  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
       "0   7       0          2            2.0          5            1.0            0.0              0              1              0              0              0              0              0              0          0         11              0              1              0        0.7        0.2   0.718070           10.0            1.0            NaN              0            1.0              4            1.0              0            0.0              1             12        2.0   0.400000   0.883679   0.370810   3.605551         0.6         0.5         0.2           3           1          10           1          10           1           5           9           1           5           8               0               1               1               0               0               1\n",
       "1   9       0          1            1.0          7            0.0            0.0              0              0              1              0              0              0              0              0          0          3              0              0              1        0.8        0.4   0.766078           11.0            1.0            NaN              0            NaN             11            1.0              1            2.0              1             19        3.0   0.316228   0.618817   0.388716   2.449490         0.3         0.1         0.3           2           1           9           5           8           1           7           3           1           1           9               0               1               1               0               1               0\n",
       "2  13       0          5            4.0          9            1.0            0.0              0              0              1              0              0              0              0              0          0         12              1              0              0        0.0        0.0        NaN            7.0            1.0            NaN              0            NaN             14            1.0              1            2.0              1             60        1.0   0.316228   0.641586   0.347275   3.316625         0.5         0.7         0.1           2           2           9           1           8           2           7           4           2           7           7               0               1               1               0               1               0\n",
       "3  16       0          0            1.0          2            0.0            0.0              1              0              0              0              0              0              0              0          0          8              1              0              0        0.9        0.2   0.580948            7.0            1.0            0.0              0            1.0             11            1.0              1            3.0              1            104        1.0   0.374166   0.542949   0.294958   2.000000         0.6         0.9         0.1           2           4           7           1           8           4           2           2           2           4           9               0               0               0               0               0               0\n",
       "4  17       0          0            2.0          0            1.0            0.0              1              0              0              0              0              0              0              0          0          9              1              0              0        0.7        0.6   0.840759           11.0            1.0            NaN              0            NaN             14            1.0              1            2.0              1             82        3.0   0.316070   0.565832   0.365103   2.000000         0.4         0.6         0.0           2           2           6           3          10           2          12           3           1           1           3               0               0               0               1               1               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop _id_ column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del d['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['target', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03',\n",
       "       'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin',\n",
       "       'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
       "       'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15',\n",
       "       'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01',\n",
       "       'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat', 'ps_car_02_cat',\n",
       "       'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat',\n",
       "       'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat',\n",
       "       'ps_car_11_cat', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14',\n",
       "       'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04',\n",
       "       'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08',\n",
       "       'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12',\n",
       "       'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin',\n",
       "       'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much missing data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595212, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.isnull().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All examples have at least one missing feature.\n",
    "What are the missing features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_reg_03',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_09_cat',\n",
       " 'ps_car_11',\n",
       " 'ps_car_12',\n",
       " 'ps_car_14']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_na = d.columns[d.isnull().any()].tolist()\n",
    "columns_with_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create dummies for all categorical variables, that __do not__ have missing values (we have to yet impute the missing values). We create dummies now, because first we will need then in main model anyway and second we will need then in models predicting missing values in other predictor columns.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in d.columns if col.endswith('_cat') and col not in columns_with_na]\n",
    "d = pd.get_dummies(d, columns=categorical_columns, drop_first=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['target', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03',\n",
       "       'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin',\n",
       "       'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
       "       'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15',\n",
       "       'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01',\n",
       "       'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat', 'ps_car_02_cat',\n",
       "       'ps_car_03_cat', 'ps_car_05_cat', 'ps_car_07_cat', 'ps_car_09_cat',\n",
       "       'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15',\n",
       "       'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04',\n",
       "       'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08',\n",
       "       'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12',\n",
       "       'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin',\n",
       "       'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin', 'ps_car_04_cat_1', 'ps_car_04_cat_2',\n",
       "       'ps_car_04_cat_3', 'ps_car_04_cat_4', 'ps_car_04_cat_5',\n",
       "       'ps_car_04_cat_6', 'ps_car_04_cat_7', 'ps_car_04_cat_8',\n",
       "       'ps_car_04_cat_9', 'ps_car_06_cat_1', 'ps_car_06_cat_2',\n",
       "       'ps_car_06_cat_3', 'ps_car_06_cat_4', 'ps_car_06_cat_5',\n",
       "       'ps_car_06_cat_6', 'ps_car_06_cat_7', 'ps_car_06_cat_8',\n",
       "       'ps_car_06_cat_9', 'ps_car_06_cat_10', 'ps_car_06_cat_11',\n",
       "       'ps_car_06_cat_12', 'ps_car_06_cat_13', 'ps_car_06_cat_14',\n",
       "       'ps_car_06_cat_15', 'ps_car_06_cat_16', 'ps_car_06_cat_17',\n",
       "       'ps_car_08_cat_1', 'ps_car_10_cat_1', 'ps_car_10_cat_2',\n",
       "       'ps_car_11_cat_2', 'ps_car_11_cat_3', 'ps_car_11_cat_4',\n",
       "       'ps_car_11_cat_5', 'ps_car_11_cat_6', 'ps_car_11_cat_7',\n",
       "       'ps_car_11_cat_8', 'ps_car_11_cat_9', 'ps_car_11_cat_10',\n",
       "       'ps_car_11_cat_11', 'ps_car_11_cat_12', 'ps_car_11_cat_13',\n",
       "       'ps_car_11_cat_14', 'ps_car_11_cat_15', 'ps_car_11_cat_16',\n",
       "       'ps_car_11_cat_17', 'ps_car_11_cat_18', 'ps_car_11_cat_19',\n",
       "       'ps_car_11_cat_20', 'ps_car_11_cat_21', 'ps_car_11_cat_22',\n",
       "       'ps_car_11_cat_23', 'ps_car_11_cat_24', 'ps_car_11_cat_25',\n",
       "       'ps_car_11_cat_26', 'ps_car_11_cat_27', 'ps_car_11_cat_28',\n",
       "       'ps_car_11_cat_29', 'ps_car_11_cat_30', 'ps_car_11_cat_31',\n",
       "       'ps_car_11_cat_32', 'ps_car_11_cat_33', 'ps_car_11_cat_34',\n",
       "       'ps_car_11_cat_35', 'ps_car_11_cat_36', 'ps_car_11_cat_37',\n",
       "       'ps_car_11_cat_38', 'ps_car_11_cat_39', 'ps_car_11_cat_40',\n",
       "       'ps_car_11_cat_41', 'ps_car_11_cat_42', 'ps_car_11_cat_43',\n",
       "       'ps_car_11_cat_44', 'ps_car_11_cat_45', 'ps_car_11_cat_46',\n",
       "       'ps_car_11_cat_47', 'ps_car_11_cat_48', 'ps_car_11_cat_49',\n",
       "       'ps_car_11_cat_50', 'ps_car_11_cat_51', 'ps_car_11_cat_52',\n",
       "       'ps_car_11_cat_53', 'ps_car_11_cat_54', 'ps_car_11_cat_55',\n",
       "       'ps_car_11_cat_56', 'ps_car_11_cat_57', 'ps_car_11_cat_58',\n",
       "       'ps_car_11_cat_59', 'ps_car_11_cat_60', 'ps_car_11_cat_61',\n",
       "       'ps_car_11_cat_62', 'ps_car_11_cat_63', 'ps_car_11_cat_64',\n",
       "       'ps_car_11_cat_65', 'ps_car_11_cat_66', 'ps_car_11_cat_67',\n",
       "       'ps_car_11_cat_68', 'ps_car_11_cat_69', 'ps_car_11_cat_70',\n",
       "       'ps_car_11_cat_71', 'ps_car_11_cat_72', 'ps_car_11_cat_73',\n",
       "       'ps_car_11_cat_74', 'ps_car_11_cat_75', 'ps_car_11_cat_76',\n",
       "       'ps_car_11_cat_77', 'ps_car_11_cat_78', 'ps_car_11_cat_79',\n",
       "       'ps_car_11_cat_80', 'ps_car_11_cat_81', 'ps_car_11_cat_82',\n",
       "       'ps_car_11_cat_83', 'ps_car_11_cat_84', 'ps_car_11_cat_85',\n",
       "       'ps_car_11_cat_86', 'ps_car_11_cat_87', 'ps_car_11_cat_88',\n",
       "       'ps_car_11_cat_89', 'ps_car_11_cat_90', 'ps_car_11_cat_91',\n",
       "       'ps_car_11_cat_92', 'ps_car_11_cat_93', 'ps_car_11_cat_94',\n",
       "       'ps_car_11_cat_95', 'ps_car_11_cat_96', 'ps_car_11_cat_97',\n",
       "       'ps_car_11_cat_98', 'ps_car_11_cat_99', 'ps_car_11_cat_100',\n",
       "       'ps_car_11_cat_101', 'ps_car_11_cat_102', 'ps_car_11_cat_103',\n",
       "       'ps_car_11_cat_104'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much classes are in balance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.963552\n",
       "1    0.036448\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.groupby('target').size() / d.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claims are filled for __3.64%__ policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Imputting missing values\n",
    "Let's go thru all features with missing values one by one, and determine simple imputation strategy that makes most sense at first glance.\n",
    "\n",
    "For every feature with missing data we will look into:\n",
    "1. Distribution of non-missing data of that features\n",
    "2. Is data _missingness_ related to filling the claim more often? (We suspect Missing At Random - MAR).\n",
    "3. Is it possible to build a model (regression for numerical and classification for categorical) to train it on filled examples and use to predict missing values?\n",
    "4. Or determine what other imputation strategy can we use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utilities that we will use while looking at data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_with_na_categorical = [c for c in columns_with_na if c.endswith('_cat')]\n",
    "columns_with_na_numeric = [c for c in columns_with_na if not c.endswith('_cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(col):\n",
    "    missing_cnt = d[d[col].isnull()].shape[0]\n",
    "    print('Missing values: {:.2f}%'.format(100 * missing_cnt / n_rows))\n",
    "    print('Missing count: {}'.format(missing_cnt))\n",
    "    if col.endswith('_cat'):\n",
    "        print(d.groupby(col).size())\n",
    "    else:\n",
    "        print(d[col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ttest(col):\n",
    "    ttest = ttest_ind(d[d[col].isnull()].target, d[d[col].notnull()].target, equal_var=False)\n",
    "    print(col + ': ' + str(ttest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillWithMean(col, df=d):\n",
    "    df.fillna({col: df[col].mean()}, inplace=True);\n",
    "    \n",
    "def fillWithMode(col, df=d):\n",
    "    df.fillna({col: df[col].mode()[0]}, inplace=True); # mode() returns a one element series\n",
    "def createFeatureForNa(col, df=d):\n",
    "    df[col+'_na'] = df[col].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression model to fill missing oridinal and continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_missing(estimator, dependent_col, df=d):\n",
    "    columns_with_na = df.columns[df.isnull().any()].tolist()\n",
    "    predictor_cols = [c for c in df.columns if c not in columns_with_na and c != dependent_col and c != 'target']\n",
    "    d_rows_na = df[df[dependent_col].isnull()]\n",
    "    d_rows_no_na = df[df[dependent_col].notnull()]\n",
    "    estimator.fit(d_rows_no_na[predictor_cols], d_rows_no_na[dependent_col])\n",
    "    predicted = estimator.predict(d_rows_na[predictor_cols])\n",
    "    cv_scores = cross_validate(estimator, d_rows_no_na[predictor_cols], d_rows_no_na[dependent_col], n_jobs=-1, return_train_score=False)['test_score']\n",
    "    return predicted, cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go thru all features one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_14']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_na_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_reg_03__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 18.11%\n",
      "Missing count: 107772\n",
      "count    487440.000000\n",
      "mean          0.894047\n",
      "std           0.345413\n",
      "min           0.061237\n",
      "25%           0.633443\n",
      "50%           0.801561\n",
      "75%           1.083974\n",
      "max           4.037945\n",
      "Name: ps_reg_03, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_reg_03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58979078,  0.58974565,  0.5908621 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, errs = predict_missing(estimator=LinearRegression(), dependent_col = 'ps_reg_03')\n",
    "errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_reg_03: Ttest_indResult(statistic=-17.084176466711412, pvalue=2.1967860501784655e-65)\n"
     ]
    }
   ],
   "source": [
    "ttest('ps_reg_03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is correlation b/w data missingness and positive class. What is the distribution of data for positive class for non-missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    18634.000000\n",
       "mean         0.952534\n",
       "std          0.362817\n",
       "min          0.253722\n",
       "25%          0.673146\n",
       "50%          0.865303\n",
       "75%          1.170937\n",
       "max          3.197753\n",
       "Name: ps_reg_03, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['target']==1]['ps_reg_03'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values are higher. But not much. Use regression model for now, for lack of better idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createFeatureForNa('ps_reg_03')\n",
    "d.loc[d['ps_reg_03'].isnull(), 'ps_reg_03'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_11__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.00%\n",
      "Missing count: 5\n",
      "count    595207.000000\n",
      "mean          2.346100\n",
      "std           0.832495\n",
      "min           0.000000\n",
      "25%           2.000000\n",
      "50%           3.000000\n",
      "75%           3.000000\n",
      "max           3.000000\n",
      "Name: ps_car_11, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is only 5 missing values. Does not matter much anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62483857,  0.62346998,  0.62301937])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, errs = predict_missing(estimator=LinearRegression(), dependent_col = 'ps_car_11')\n",
    "errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_car_11: Ttest_indResult(statistic=-150.04856101232082, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "ttest('ps_car_11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is not accurate, but let's fill from regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createFeatureForNa('ps_car_11')\n",
    "d.loc[d['ps_car_11'].isnull(), 'ps_car_11'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_12__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.00%\n",
      "Missing count: 1\n",
      "count    595211.000000\n",
      "mean          0.379947\n",
      "std           0.058300\n",
      "min           0.100000\n",
      "25%           0.316228\n",
      "50%           0.374166\n",
      "75%           0.400000\n",
      "max           1.264911\n",
      "Name: ps_car_12, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 1 missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86847553,  0.87188254,  0.87125443])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted, errs = predict_missing(estimator=LinearRegression(), dependent_col = 'ps_car_12')\n",
    "errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite good regression accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createFeatureForNa('ps_car_12')\n",
    "d.loc[d['ps_car_12'].isnull(), 'ps_car_12'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_reg_14__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 7.16%\n",
      "Missing count: 42620\n",
      "count    552592.000000\n",
      "mean          0.374691\n",
      "std           0.045610\n",
      "min           0.109545\n",
      "25%           0.350428\n",
      "50%           0.373497\n",
      "75%           0.398121\n",
      "max           0.636396\n",
      "Name: ps_car_14, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted, errs = predict_missing(estimator=LinearRegression(), dependent_col = 'ps_car_14')\n",
    "# errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very poor regression accuracy, fill with mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createFeatureForNa('ps_car_14')\n",
    "fillWithMean('ps_car_14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's go thru all categorical  features missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_09_cat']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_na_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_ind_02_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.04%\n",
      "Missing count: 216\n",
      "ps_ind_02_cat\n",
      "1.0    431859\n",
      "2.0    123573\n",
      "3.0     28186\n",
      "4.0     11378\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_ind_02_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  2.  2.  1.  1.  1.  1.  1.\n",
      "  2.  1.  1.  2.  2.  1.  1.  2.  1.  2.  1.  1.  1.  1.  1.  1.  1.  2.\n",
      "  1.  1.  1.  1.  1.  2.  1.  1.  1.  2.  2.  1.  1.  1.  1.  1.  2.  1.\n",
      "  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  1.  1.  2.  2.\n",
      "  1.  1.  2.  1.  1.  1.  1.  1.  2.  2.  1.  1.  1.  2.  1.  1.  1.  1.\n",
      "  1.  2.  1.  1.  1.  1.  1.  1.  1.  2.  2.  1.  2.  1.  2.  1.  1.  1.\n",
      "  1.  1.  2.  1.  2.  2.  1.  1.  1.  2.  2.  1.  1.  1.  2.  1.  1.  1.\n",
      "  2.  1.  2.  1.  1.  1.  2.  1.  2.  1.  1.  1.  1.  1.  2.  1.  1.  1.\n",
      "  2.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.\n",
      "  1.  1.  1.  1.  1.  1.  1.  2.  2.  1.  1.  1.  1.  1.  1.  2.  1.  2.\n",
      "  2.  1.  1.  1.  1.  1.  2.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.] [ 0.75956094  0.75861182  0.75787446]\n"
     ]
    }
   ],
   "source": [
    "predicted, errs = predict_missing(RandomForestClassifier(n_jobs=-1), 'ps_ind_02_cat')\n",
    "print(predicted, errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the distribution for positive target class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ps_ind_02_cat\n",
       "1.0    15428\n",
       "2.0     4713\n",
       "3.0     1049\n",
       "4.0      464\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['target']==1].groupby('ps_ind_02_cat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively much target truths for NAs\n",
    "createFeatureForNa('ps_ind_02_cat')\n",
    "d.loc[d['ps_ind_02_cat'].isnull(), 'ps_ind_02_cat'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_ind_04_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.01%\n",
      "Missing count: 83\n",
      "ps_ind_04_cat\n",
      "0.0    346965\n",
      "1.0    248164\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_ind_04_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ps_ind_04_cat\n",
       "0.0    12071\n",
       "1.0     9590\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['target']==1].groupby('ps_ind_04_cat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.\n",
      "  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.7181024   0.71885208  0.71841352]\n"
     ]
    }
   ],
   "source": [
    "predicted, errs = predict_missing(RandomForestClassifier(n_jobs=-1), 'ps_ind_04_cat')\n",
    "print(predicted, errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier accuracy was poor, filling with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively much target truths for NAs\n",
    "createFeatureForNa('ps_ind_04_cat')\n",
    "d.loc[d['ps_ind_04_cat'].isnull(), 'ps_ind_04_cat'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_ind_05_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.98%\n",
      "Missing count: 5809\n",
      "ps_ind_05_cat\n",
      "0.0    528009\n",
      "1.0      8322\n",
      "2.0      4184\n",
      "3.0      8233\n",
      "4.0     18344\n",
      "5.0      1649\n",
      "6.0     20662\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_ind_05_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.] [ 0.89588741  0.89619684  0.89649047]\n"
     ]
    }
   ],
   "source": [
    "predicted, errs = predict_missing(RandomForestClassifier(n_jobs=-1), 'ps_ind_05_cat')\n",
    "print(predicted, errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively much target truths for NAs\n",
    "createFeatureForNa('ps_ind_05_cat')\n",
    "d.loc[d['ps_ind_05_cat'].isnull(), 'ps_ind_05_cat'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_01_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.02%\n",
      "Missing count: 107\n",
      "ps_car_01_cat\n",
      "0.0       5904\n",
      "1.0       1367\n",
      "2.0       2144\n",
      "3.0       6658\n",
      "4.0      26174\n",
      "5.0      18142\n",
      "6.0      62393\n",
      "7.0     179247\n",
      "8.0      15093\n",
      "9.0      20323\n",
      "10.0     50087\n",
      "11.0    207573\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_01_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted, errs = predict_missing(RandomForestClassifier(n_jobs=-1), 'ps_car_01_cat')\n",
    "# print(predicted, errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very poor accuracy, fill with mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively much target truths for NAs\n",
    "createFeatureForNa('ps_car_01_cat')\n",
    "fillWithMode('ps_car_01_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_02_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.00%\n",
      "Missing count: 5\n",
      "ps_car_02_cat\n",
      "0.0    101217\n",
      "1.0    493990\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_02_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  1.  0.] [ 0.93909366  0.94088769  0.93946634]\n"
     ]
    }
   ],
   "source": [
    "predicted, errs = predict_missing(RandomForestClassifier(n_jobs=-1), 'ps_car_02_cat')\n",
    "print(predicted, errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createFeatureForNa('ps_car_02_cat')\n",
    "d.loc[d['ps_car_02_cat'].isnull(), 'ps_car_02_cat'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_03_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 69.09%\n",
      "Missing count: 411231\n",
      "ps_car_03_cat\n",
      "0.0     73272\n",
      "1.0    110709\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_03_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most data is missing here. What is data distribution in positive target class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ps_car_03_cat\n",
       "0.0    2897\n",
       "1.0    5347\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['target']==1].groupby('ps_car_03_cat').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems very influencing..\n",
    "Are we able to build any reliable classifier from other features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  1. ...,  0.  0.  1.] [ 0.78342329  0.78211881  0.78360265]\n"
     ]
    }
   ],
   "source": [
    "predicted, errs = predict_missing(RandomForestClassifier(n_jobs=-1), 'ps_car_03_cat')\n",
    "print(predicted, errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createFeatureForNa('ps_car_03_cat')\n",
    "d.loc[d['ps_car_03_cat'].isnull(), 'ps_car_03_cat'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_05_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 44.78%\n",
      "Missing count: 266551\n",
      "ps_car_05_cat\n",
      "0.0    155994\n",
      "1.0    172667\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_05_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ps_car_05_cat\n",
       "0.0    6257\n",
       "1.0    6985\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['target']==1].groupby('ps_car_05_cat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  0. ...,  1.  0.  1.] [ 0.56982858  0.57185498  0.56865627]\n"
     ]
    }
   ],
   "source": [
    "predicted, errs = predict_missing(DecisionTreeClassifier(), 'ps_car_05_cat')\n",
    "print(predicted, errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del d['ps_car_05_cat']\n",
    "createFeatureForNa('ps_car_05_cat')\n",
    "d.loc[d['ps_car_05_cat'].isnull(), 'ps_car_05_cat'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_07_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 1.93%\n",
      "Missing count: 11489\n",
      "ps_car_07_cat\n",
      "0.0     30575\n",
      "1.0    553148\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_07_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  1.  1.  1.] [ 0.95125273  0.95158679  0.95288144]\n"
     ]
    }
   ],
   "source": [
    "predicted, errs = predict_missing(RandomForestClassifier(n_jobs=-1), 'ps_car_07_cat')\n",
    "print(predicted, errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively much target truths for NAs\n",
    "createFeatureForNa('ps_car_07_cat')\n",
    "d.loc[d['ps_car_07_cat'].isnull(), 'ps_car_07_cat'] = predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ps_car_09_cat__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.10%\n",
      "Missing count: 569\n",
      "ps_car_09_cat\n",
      "0.0    194518\n",
      "1.0     29080\n",
      "2.0    353482\n",
      "3.0     14756\n",
      "4.0      2807\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_stats('ps_car_09_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted, errs = predict_missing(DecisionTreeClassifier(), 'ps_car_09_cat')\n",
    "# errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor classifier accuracy, fill with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively much target truths for NAs\n",
    "createFeatureForNa('ps_car_09_cat')\n",
    "fillWithMode('ps_car_09_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have we handled all missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_with_na = d.columns[d.isnull().any()].tolist()\n",
    "assert not columns_with_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create dumies also for the categorical variables that had missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d = pd.get_dummies(d, columns=[c for c in columns_with_na_categorical if c != 'ps_car_05_cat'], drop_first=True);\n",
    "d = pd.get_dummies(d, columns=columns_with_na_categorical, drop_first=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_features = [c for c in d.columns if '_bin' in c]\n",
    "binary_ind_features = [c for c in binary_features if '_ind_' in c]\n",
    "binary_calc_features = [c for c in binary_features if '_calc_' in c]\n",
    "categorical_features = [c for c in d.columns if '_cat' in c and '_calc_' not in c]\n",
    "numeric_features = [c for c in d.columns if c not in binary_features and c not in categorical_features and '_calc_' not in c and c!='target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating  single classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model performance metric will be Gini index. \n",
    "\n",
    "However we cannot pass that function to cross_validate / grid_search routines, as that routines run in parallel and it is not possible to pickle a function (without much complication). Instead our target metric will be just Area Under ROC, as gini index is directly proportional to it. Only at the very end we will compute gini normalized, in one thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    fpr, tpr, thr = roc_curve(y, pred, pos_label=1)\n",
    "    g = 2 * auc(fpr, tpr) -1\n",
    "    return g\n",
    "\n",
    "def gini_norm(y, pred):\n",
    "    normed = gini(y, pred) / gini(y,y)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = binary_ind_features + categorical_features + numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = d[features]\n",
    "y = d['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier = Pipeline([\n",
    "#     ('pca', PCA()),\n",
    "#     ('tree', RandomForestClassifier(n_estimators=50, class_weight = 'balanced'))\n",
    "# ])\n",
    "\n",
    "# grid_search_CV = GridSearchCV(classifier, {\n",
    "#     'pca__n_components': [25, 50, 100],\n",
    "#     'tree__min_samples_split': [1000, 5000, 10000],\n",
    "# }, n_jobs=7, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring=make_scorer(roc_auc_score), verbose=10)\n",
    "\n",
    "# grid_search_CV.fit(X, y)\n",
    "\n",
    "# grid_search_CV.best_params_, grid_search_CV.best_score_\n",
    "\n",
    "# # {'pca__n_components': 25,\n",
    "# #  'tree__class_weight': 'balanced',\n",
    "# #  'tree__min_samples_split': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier = Pipeline([\n",
    "#     ('tree', RandomForestClassifier(n_estimators=50, class_weight = 'balanced', criterion='entropy', min_samples_split=5000))\n",
    "# ])\n",
    "\n",
    "# grid_search_CV = GridSearchCV(classifier, {\n",
    "#     'tree__min_samples_split': [3000, 5000, 7000, 9000],\n",
    "#     'tree__max_features': [13, 14, 15]\n",
    "# }, n_jobs=7, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring=make_scorer(roc_auc_score), verbose=10)\n",
    "\n",
    "# grid_search_CV.fit(X, y)\n",
    "\n",
    "# grid_search_CV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=3, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgboost.DMatrix(dtrain[predictors].values, label=dtrain['target'].values)\n",
    "        cvresult = xgboost.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['target'],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"Accuracy : %.4g\" % accuracy_score(dtrain['target'].values, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % roc_auc_score(dtrain['target'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# xgb1 = XGBClassifier(learning_rate=0.05, n_estimators=1000, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "\n",
    "# modelfit(xgb1, d, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid_search_CV = GridSearchCV(XGBClassifier(learning_rate=0.2, n_estimators=150, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic'), {\n",
    "#     'reg_alpha':[20, 50, 150]\n",
    "# }, n_jobs=-1, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring='roc_auc', verbose=10)\n",
    "\n",
    "# grid_search_CV.fit(X, y)\n",
    "\n",
    "# grid_search_CV.best_params_, grid_search_CV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "# cross_validate(classifier, X, y, n_jobs=1, cv=StratifiedKFold(n_splits=4, shuffle=True), scoring=make_scorer(gini_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# classifier = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = classifier.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gini_norm(y_test, y_pred)\n",
    "# # 0.2844"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minority class oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# X_resampled, y_resampled = RandomOverSampler().fit_sample(X_train, y_train)\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns = X_train.columns)\n",
    "# y_resampled_df = pd.Series(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out RF with random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1)\n",
    "# classifier.fit(X_resampled_df, y_resampled_df)\n",
    "\n",
    "# y_pred = classifier.predict_proba(X_test)[:,1]\n",
    "# gini_norm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Far better than RF on unbalanced dataset (which had Gini of 0.18). To be used in further ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# X_resampled, y_resampled = RandomUnderSampler().fit_sample(X_train, y_train)\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns = X_train.columns)\n",
    "# y_resampled_df = pd.Series(y_resampled)\n",
    "\n",
    "# classifier = RandomForestClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1)\n",
    "# classifier.fit(X_resampled_df, y_resampled_df)\n",
    "\n",
    "# y_pred = classifier.predict_proba(X_test)[:,1]\n",
    "# gini_norm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad 0.2315 vs 0.19 baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also SMOTE and ADASYN was tried out on Random Forest, but with poor results. Baseline RF performance (Gini) was 0.19, while with SMOTE 0.189 and with ADASYN 0168. No under or oversampling techniques helped XGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# classifier = ExtraTreesClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1, max_features=14)\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = classifier.predict_proba(X_test)[:,1]\n",
    "# gini_norm(y_test, y_pred)\n",
    "# # 0.2557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra trees Gini 0.2557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# xgb = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "# xgb.fit(X_train, y_train)\n",
    "\n",
    "# X_resampled, y_resampled = RandomOverSampler().fit_sample(X_train, y_train)\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns = X_train.columns)\n",
    "# y_resampled_df = pd.Series(y_resampled)\n",
    "# rf = RandomForestClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1)\n",
    "# rf.fit(X_resampled_df, y_resampled_df)\n",
    "\n",
    "# xtrees = ExtraTreesClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1, max_features=14)\n",
    "# xtrees.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_pred_xgb = xgb.predict_proba(X_test)[:,1]\n",
    "# y_pred_rf = rf.predict_proba(X_test)[:,1]\n",
    "# y_pred_xtrees = xtrees.predict_proba(X_test)[:,1]\n",
    "# y_pred_xgb_resampled = xgb_resampled.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to:\n",
    "1. Train couple of possibly strong classifiers.\n",
    "2. Check correlations among their predictions\n",
    "3. Check correlations among their feature importances\n",
    "2. Use stacking (out of fold predictions on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgb = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "# xgb.fit(X_train, y_train)\n",
    "# xgb_y_pred = classifier.predict_proba(X_test)[:,1]\n",
    "# pd.Series(xgb_y_pred).to_csv('tmp' + os.sep + 'validation_set_preditions_xgb.csv', float_format='%.4f', index=False)\n",
    "# gini_norm(y_test, xgb_y_pred)\n",
    "# 0.2838"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# xtrees = ExtraTreesClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1, max_features=14)\n",
    "# xtrees.fit(X_train, y_train)\n",
    "# xtrees_y_pred = xtrees.predict_proba(X_test)[:,1]\n",
    "# pd.Series(xtrees_y_pred).to_csv('tmp' + os.sep + 'validation_set_preditions_xtrees.csv', float_format='%.4f', index=False)\n",
    "# gini_norm(y_test, xtrees_y_pred)\n",
    "# 0.25610"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_resampled, y_resampled = RandomOverSampler().fit_sample(X_train, y_train)\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns = X_train.columns)\n",
    "# y_resampled_df = pd.Series(y_resampled)\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1)\n",
    "# rf.fit(X_resampled_df, y_resampled_df)\n",
    "# rf_y_pred = rf.predict_proba(X_test)[:,1]\n",
    "# pd.Series(rf_y_pred).to_csv('tmp' + os.sep + 'validation_set_preditions_rf.csv', float_format='%.4f', index=False)\n",
    "# gini_norm(y_test, rf_y_pred)\n",
    "# 0.26627"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# gb = GradientBoostingClassifier(min_samples_split=300, n_estimators=300)\n",
    "# gb.fit(X_train, y_train)\n",
    "# gb_y_pred = gb.predict_proba(X_test)[:,1]\n",
    "# pd.Series(gb_y_pred).to_csv('tmp' + os.sep + 'validation_set_preditions_gb.csv', float_format='%.4f', index=False)\n",
    "# gini_norm(y_test, gb_y_pred)\n",
    "# # # 0.27957 - best but takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "xtrees = ExtraTreesClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1, max_features=14)\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1) #TODO add oversampling\n",
    "gb = GradientBoostingClassifier(min_samples_split=300, n_estimators=100) #TODO change n_estimators=300\n",
    "\n",
    "lr = LogisticRegression() #TODO: other classifier? Tree? regularize regression?\n",
    "\n",
    "# The StackingCVClassifier uses scikit-learn's check_cv\n",
    "# internally, which doesn't support a random seed. Thus\n",
    "# NumPy's random seed need to be specified explicitely for\n",
    "# deterministic behavior\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, scoring=make_scorer(roc_auc_score), n_jobs=1):\n",
    "    \"\"\"\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_learning_curve(rf, \"Learning Curves for RF, most tuned settings\", X, y, ylim=(0.01, 1.01), n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['tuned RF learning curve'](img/tuned_rf_learning_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['tuned XGB learning curve'](img/tuned_xgb_learning_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['tuned xtrees learning curve'](img/tuned_xtrees_learning_curve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=lr, use_probas=True, verbose=2) # TODO cv=2 is the default - enlarge, up to 5?\n",
    "# #TODO: stratify=True\n",
    "# #TODO grid search CV parameters of level 1 classifiers, as working in such ensemble?\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=lr, use_probas=True, stratify=True, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2761 - no point to add StackingCVClassifier(stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=RandomForestClassifier(n_estimators=100, n_jobs=-1), use_probas=True, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.1441 !!!! overfit. Stay with normal linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=lr, use_probas=True, cv=3, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2771: better than default (StackingCVClassifier.cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_resampled, y_resampled = RandomOverSampler().fit_sample(X_train, y_train)\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns = X_train.columns)\n",
    "# y_resampled_df = pd.Series(y_resampled)\n",
    "\n",
    "# xgb = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                              subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "# xtrees = ExtraTreesClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1, max_features=14)\n",
    "# rf = RandomForestClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1)\n",
    "# gb = GradientBoostingClassifier(min_samples_split=300, n_estimators=300)\n",
    "\n",
    "# # TODO: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html: tune:\n",
    "# # - regularization\n",
    "# # - class_weight\n",
    "# # ponoc tez Logistic regression zachowuje sie lepiej na normalnych rozkladach...Zobaczyc na rozklady.\n",
    "# lr = LogisticRegression()\n",
    "\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=lr, use_probas=True, cv=3, verbose=2) \n",
    "\n",
    "# sclf.fit(X_resampled_df.values, y_resampled_df.values)\n",
    "# sclf_y_pred = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred)\n",
    "# # 0.23538 and takes very long (2h?)s\n",
    "# # Bad idea to use all base models on oversampled train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversample only for RF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class OverSamplingRandomForestClassifier(RandomForestClassifier):\n",
    "#     def fit(self, X, y, sample_weight=None):\n",
    "#         X_resampled, y_resampled = RandomOverSampler().fit_sample(X, y)\n",
    "#         return super(OverSamplingRandomForestClassifier, self).fit(X_resampled,y_resampled,sample_weight)\n",
    "\n",
    "# xgb = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "#                                              subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "# xtrees = ExtraTreesClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1, max_features=14)\n",
    "# rf = OverSamplingRandomForestClassifier(n_estimators=100, class_weight = 'balanced', criterion='entropy', min_samples_split=5000, n_jobs=-1)\n",
    "# gb = GradientBoostingClassifier(min_samples_split=300, n_estimators=100) # TODO increase to 300\n",
    "# lr = LogisticRegression()\n",
    "\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=lr, use_probas=True, cv=3, verbose=2) \n",
    "\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred)\n",
    "# # 0.27707 - no gain from oversampling RF (while in this stacked generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about CV 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=lr, use_probas=True, cv=5, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2777: not much better than cv 3..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some more regularization for Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/4)\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0.01, learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=2, missing=None, n_estimators=750, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=10, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier2: extratreesclassifier (2/4)\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='entropy', max_depth=None, max_features=14,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=5000, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier3: randomforestclassifier (3/4)\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='entropy', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=5000, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier4: gradientboostingclassifier (4/4)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27179085657475133"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=LogisticRegression(C=0.1), use_probas=True, cv=3, verbose=2)\n",
    "sclf.fit(X_train.values, y_train.values)\n",
    "sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "gini_norm(y_test, sclf_y_pred_proba)\n",
    "# 0.2710, 0.2718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.03240735,  1.87040263, -1.02695668,  0.86424144, -1.35974774,\n",
       "         1.2012823 , -1.11801293,  0.95600837]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.meta_clf_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16200456])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.meta_clf_.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8], dtype=int32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.meta_clf_.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__max_iter=300\n",
    "class_weight='balanced'\n",
    "penalty='l1'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/4)\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0.01, learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=2, missing=None, n_estimators=750, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=10, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier2: extratreesclassifier (2/4)\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='entropy', max_depth=None, max_features=14,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=5000, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier3: randomforestclassifier (3/4)\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='entropy', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=5000, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier4: gradientboostingclassifier (4/4)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27881835670464339"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=LogisticRegression(max_iter=300, class_weight='balanced'), use_probas=True, cv=3, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2788"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use_features_in_secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=LogisticRegression(), use_probas=True, cv=3, use_features_in_secondary=True, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# 0.2745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How about use features in secondary and XGB as metaclassifier you will have to tune it perhaps__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/4)\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0.01, learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=2, missing=None, n_estimators=750, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=10, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.9)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier2: extratreesclassifier (2/4)\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='entropy', max_depth=None, max_features=14,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=5000, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier3: randomforestclassifier (3/4)\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='entropy', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=5000, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier4: gradientboostingclassifier (4/4)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28044973164797127"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_xgb = XGBClassifier(learning_rate=0.05, n_estimators=750, max_depth=3, min_child_weight=2, gamma=0.01, reg_alpha=10,\n",
    "                                             subsample=0.9, colsample_bytree=0.9, objective='binary:logistic')\n",
    "sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=meta_xgb, use_probas=True, cv=3, use_features_in_secondary=True, verbose=2)\n",
    "sclf.fit(X_train.values, y_train.values)\n",
    "sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "gini_norm(y_test, sclf_y_pred_proba)\n",
    "# 0.2804"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other KFold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=LogisticRegression(), use_probas=True, cv=StratifiedKFold(n_splits=3, shuffle=True), verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2757 - worse than plain cv = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and less regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf, gb], meta_classifier=LogisticRegression(C=10), use_probas=True, cv=3, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, xtrees, rf], meta_classifier=lr, use_probas=True, cv=3, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgb.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = xgb.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb], meta_classifier=lr, use_probas=True, cv=3, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sclf = StackingCVClassifier(classifiers=[xgb, rf], meta_classifier=lr, use_probas=True, cv=3, verbose=2)\n",
    "# sclf.fit(X_train.values, y_train.values)\n",
    "# sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "# gini_norm(y_test, sclf_y_pred_proba)\n",
    "# # 0.2769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sclf = StackingCVClassifier(classifiers=[xgb, gb], meta_classifier=lr, use_probas=True, cv=3, verbose=2)\n",
    "sclf.fit(X_train.values, y_train.values)\n",
    "sclf_y_pred_proba = sclf.predict_proba(X_test.values)[:,1]\n",
    "gini_norm(y_test, sclf_y_pred_proba)\n",
    "# 0.2842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = pd.read_csv('datasets' + os.sep + 'test.csv', na_values=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892816, 217)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values imputation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = d_test['id']\n",
    "del d_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = pd.get_dummies(d_test, columns=categorical_columns, drop_first=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillFromModel(colname, df, estimator):\n",
    "    if df[colname].isnull().values.any():\n",
    "        predicted, _ = predict_missing(estimator, dependent_col = colname, df=df)\n",
    "        df.loc[d_test[colname].isnull(), colname] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createFeatureForNa('ps_reg_03', d_test)\n",
    "createFeatureForNa('ps_car_11', d_test)\n",
    "createFeatureForNa('ps_car_12', d_test)\n",
    "\n",
    "createFeatureForNa('ps_car_14', d_test)\n",
    "createFeatureForNa('ps_ind_02_cat', d_test)\n",
    "createFeatureForNa('ps_ind_04_cat', d_test)\n",
    "\n",
    "createFeatureForNa('ps_ind_05_cat', d_test)\n",
    "\n",
    "createFeatureForNa('ps_car_01_cat', d_test)\n",
    "\n",
    "createFeatureForNa('ps_car_02_cat', d_test)\n",
    "createFeatureForNa('ps_car_03_cat', d_test)\n",
    "\n",
    "# del d_test['ps_car_05_cat']\n",
    "createFeatureForNa('ps_car_05_cat', d_test)\n",
    "\n",
    "createFeatureForNa('ps_car_07_cat', d_test)\n",
    "\n",
    "createFeatureForNa('ps_car_09_cat', d_test)\n",
    "\n",
    "\n",
    "fillFromModel('ps_reg_03', d_test, LinearRegression())\n",
    "fillFromModel('ps_car_11', d_test, LinearRegression())\n",
    "fillFromModel('ps_car_12', d_test, LinearRegression())\n",
    "\n",
    "fillWithMean('ps_car_14', d_test)\n",
    "fillWithMode('ps_ind_02_cat', d_test)\n",
    "fillWithMode('ps_ind_04_cat', d_test)\n",
    "\n",
    "fillFromModel('ps_ind_05_cat', d_test, DecisionTreeClassifier())\n",
    "\n",
    "fillWithMode('ps_car_01_cat', d_test)\n",
    "\n",
    "fillFromModel('ps_car_02_cat', d_test, DecisionTreeClassifier())\n",
    "fillFromModel('ps_car_03_cat', d_test, DecisionTreeClassifier())\n",
    "\n",
    "fillFromModel('ps_car_05_cat', d_test, DecisionTreeClassifier())\n",
    "\n",
    "fillFromModel('ps_car_07_cat', d_test, DecisionTreeClassifier())\n",
    "\n",
    "fillWithMode('ps_car_09_cat', d_test)\n",
    "\n",
    "d_test = pd.get_dummies(\n",
    "    d_test,\n",
    "    columns=columns_with_na_categorical,\n",
    "    drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any other NAs in test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_columns_with_na = d_test.columns[d_test.isnull().any()].tolist()\n",
    "assert not test_set_columns_with_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test and output submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['target'] = classifier.predict_proba(d_test[features])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_time_stamp = datetime.now().strftime('%Y_%m_%d__%H_%M_%S')\n",
    "submission.to_csv('submissions' + os.sep + 'submission_' + date_time_stamp + '.csv', float_format='%.4f', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
